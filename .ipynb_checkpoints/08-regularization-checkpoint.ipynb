{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 8: Regularization\n",
    "\n",
    "## Reading: Bishop 5.5\n",
    "\n",
    "## 1 Motivating regularization\n",
    "\n",
    "“YEAH, BUT YOUR SCIENTISTS WERE SO PREOCCUPIED WITH WHETHER OR NOT THEY COULD THAT THEY DIDN’T STOP TO THINK IF THEY SHOULD.” -- Dr. Ian Malcolm\n",
    "\n",
    "<img src=\"images/goldblum.jpg\">\n",
    "\n",
    "\n",
    "We now have the power to create functions (namely neural networks) that have the power to approximate any other function, given a sufficient number of parameters.  However, as we learned when we were fitting polynomials to curves all the way back at the start of the class, unlimited model complexity is a path fraught with peril.  Recall that if we have a simple dataset like this one:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create constantly-spaced x-values\n",
    "x = np.linspace(0,1,11)\n",
    "\n",
    "# Create a linear function of $x$ with slope 1, intercept 1, and normally distributed error with sd=1\n",
    "y = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "y_test = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "plt.plot(x,y,'k.')\n",
    "plt.plot(x,y_test,'r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit arbitrarily complex models such that we hit the training data exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.')\n",
    "x_smooth = np.linspace(0,1,101)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "degrees = range(1,13,2)\n",
    "for d in degrees:\n",
    "    X = np.vander(x,d,increasing=True)\n",
    "    w = np.linalg.solve(X.T @ X,X.T@y)\n",
    "    y_pred = X@w\n",
    "    plt.plot(x_smooth,np.vander(x_smooth,d,increasing=True)@w)\n",
    "    training_error = 1./len(y)*np.sum((y_pred-y)**2)\n",
    "    test_error = 1./len(y_test)*np.sum((y_pred-y_test)**2)\n",
    "    train_errors.append(training_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then plot the resulting training set and test errors as a function of the number of degrees of freedom, we see this typical pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees,train_errors,label='Training error')\n",
    "plt.plot(degrees,test_errors,label='Test error')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error decreases with increased model complexity, while the test error initially decreases, then begins to *increase* as the model becomes more complex.\n",
    "\n",
    "Of course this very same thing can happen in neural networks (linear regression is, after all, a very simple version of a multilayer perceptron with no hidden layers and the identity as an activation function).  In the case of linear regression, we made the simple choice to just limit our model complexity.  This is certainly possible with neural nets, by limiting the number of hidden layers and nodes.  However, it's a little bit more challenging to decide just exactly *where* overfitting is coming from, and to tailor the network in response.  Instead, we introduce a technique that we will refer to as *regularization*.  Regularization is broadly understood to be a technique that trades training set accuracy for test set accuracy, and there are a multitude of flavors: we'll explore a few of them here.  \n",
    "\n",
    "## 2 $L_2$ Regularization\n",
    "The most common idea for regularizing has been around for a very long time, and it results from the observation that in most regression problems, large weights tend to correspond to overfitting the data.  As such, we can make an effort to reduce overfitting by explicitly penalizing large parameters in the cost function.  In particular, we can simply add the following term:\n",
    "$$\n",
    "\\mathcal{L}' = \\underbrace{\\mathcal{L}}_{\\text{Sum Square Error}} + \\frac{\\gamma}{2}\\; \\sum_{l=1}^{L} \\|W^{(l)}\\|_{2}^2,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\|W^{(l)}\\|_2^2 = \\sum_{i} \\sum_{j} (w_{ij}^{(l)})^2\n",
    "$$\n",
    "is the square of the *Frobenius* norm, which generalizes the normal $L_2$ norm (aka Euclidean distance) to matrices.\n",
    "\n",
    "## IC8A\n",
    "$L_2$ regularization has many names including ridge regression and Tikhonov regularization.  However, in the world of machine learning, it is often called **weight decay**.  Compute the derivative of $\\frac{\\gamma}{2}\\; \\sum_{l=1}^L \\|W^{(l)}\\|_2^2$ with respect to some arbitrary weight $w_{ij}^{(l)}$ (ignore the misfit component of the cost function for now), and determine the resulting gradient descent update formula, e.g.\n",
    "$$\n",
    "w_{ij}^{(l)} \\leftarrow w_{ij}^{(l)} - \\ldots\n",
    "$$\n",
    "**Why is it called weight decay?**\n",
    "\n",
    "## 2a $L_2$ Regularization for linear regression\n",
    "The result derived above is general, but when applied to the linear regression problem we can write down the closed form solution for the optimal parameters as\n",
    "$$\n",
    "(X^T X + \\gamma \\mathcal{I}) \\mathbf{w} = X^T y,\n",
    "$$\n",
    "where $\\mathcal{I}$ is an appropriately sized identity matrix (The additive term along the matrix diagonal gives rise to the name *ridge regression*).  **What does the parameter $\\gamma$ do**?\n",
    "\n",
    "We can easily implement this for a high-dimensional problem, and explore what happens to our model fit as we adjust $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.')\n",
    "x_smooth = np.linspace(0,1,101)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "d = 13\n",
    "gammas = np.logspace(-7,1,12)\n",
    "for gamma in gammas:\n",
    "    X = np.vander(x,d,increasing=True)\n",
    "    identity = np.eye(X.shape[1])\n",
    "    identity[0,0] = 0 # Why do I do this?\n",
    "    w = np.linalg.solve(X.T @ X + gamma*identity,X.T@y)\n",
    "    y_pred = X@w\n",
    "    plt.plot(x_smooth,np.vander(x_smooth,d,increasing=True)@w)\n",
    "    training_error = 1./len(y)*np.sum((y_pred-y)**2)\n",
    "    test_error = 1./len(y_test)*np.sum((y_pred-y_test)**2)\n",
    "    train_errors.append(training_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can look at the test and training accuracies together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(gammas,train_errors,label='Training error')\n",
    "plt.semilogx(gammas,test_errors,label='Test error')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As similar picture emerges (although flipped around the x-axis, because a larger $\\gamma$ corresponds to a simpler model.  \n",
    "\n",
    "## 2b $L_2$ Regularization for neural networks\n",
    "\n",
    "As it turns out, this is simple to apply to neural networks as well.  To illustrate its effect, let's synthesize a very noisy dataset to be used for a classification problem.  This is not dissimilar to the iris data set, in the sense that there are classes that overlap one another in their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y = make_moons(n_samples=300,noise=0.4)\n",
    "\n",
    "X,X_test,y,y_test = train_test_split(X,y)\n",
    "X0,y0 = X.copy(),y.copy()\n",
    "X0_test,y0_test = X_test.copy(),y_test.copy()\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=y)\n",
    "plt.scatter(X_test[:,0],X_test[:,1],c=y_test,marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear pattern here, but it is quite noisy.  Let's see if we can fit a very flexible neural network to this dataset using pytorch.  We'll first go through our ritual of converting the data into the appropriate type and location, and then create a DataLoader for use with batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our neural network.  It'll be a simple affair with a single hidden layer, but plenty of nodes to ensure a flexible function.  Something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(2,2048) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(2048,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.relu(a1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can optimize.  Let's pay attention to the training and test set accuracy as we optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 3000\n",
    "# Loop over the data\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    train_accs.append(100.*correct_train/total_train)\n",
    "    test_accs.append(100.*correct/total)\n",
    "    if epoch%10==0:\n",
    "        print(epoch,loss.item(),train_accs[-1],test_accs[-1])\n",
    "plt.plot(train_accs,label='Training accuracy')\n",
    "plt.plot(test_accs,label='Test accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above case exhibits the classic symptoms of overfitting.  How do you know?  Based on the plot above, can you identify an alternative method of regularization?**\n",
    "\n",
    "This neural network only has two features, so we can easily visualize its predictions as a grid. Let's see what the decision boundary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0grid,X1grid = np.meshgrid(np.linspace(X[:,0].cpu().min(),X[:,0].cpu().max(),101),np.linspace(X[:,1].cpu().min(),X[:,1].cpu().max(),101))\n",
    "\n",
    "X_grid = np.vstack((X0grid.ravel(),X1grid.ravel())).T\n",
    "X_grid = torch.from_numpy(X_grid)\n",
    "X_grid = X_grid.to(torch.float32)\n",
    "X_grid = X_grid.cuda()\n",
    "\n",
    "t = model(X_grid)\n",
    "out = F.softmax(t,dim=1)\n",
    "plt.contourf(X0grid,X1grid,out.cpu().detach().numpy()[:,1].reshape((101,101)),alpha=0.5)\n",
    "plt.scatter(X0[:,0],X0[:,1],c=y0,label='Training Data')\n",
    "plt.scatter(X0_test[:,0],X0_test[:,1],c=y0_test,marker='x',label='Test Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the problem here?**\n",
    "\n",
    "We can allay this issue using $L_2$ regularization.  How do we implement this?  As it turns out, it's not so bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "gamma = 0.1  ### HERE'S THE REGULARIZATION PARAMETER!\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "epochs = 3000\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        \n",
    "        ### HERE'S WHERE WE ADD REGULARIZATION!\n",
    "        for W in model.parameters():\n",
    "            # Loop over all the model parameters\n",
    "            if W.dim()>1:\n",
    "                # Loop over all the weight matrices (but not the biases)\n",
    "                loss += gamma/(2*d.shape[0])*(W**2).sum()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        # NOTE THAT THIS NOW INCLUDES A DIRECT DEPENDENCY ON THE PARAMETERS DUE TO THE REGULARIZATION TERM\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    train_accs.append(100.*correct_train/total_train)\n",
    "    test_accs.append(100.*correct/total)\n",
    "    if epoch%100==0:\n",
    "        print(epoch,loss.item(),train_accs[-1],test_accs[-1])\n",
    "        \n",
    "plt.plot(train_accs,label='Training accuracy')\n",
    "plt.plot(test_accs,label='Test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does adding this regularization improve the overfitting issues?  How do you know?**\n",
    "\n",
    "**Critically, what happens if we increase the regularization parameter too far?**\n",
    "\n",
    "Let's print the resulting decision surface below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0grid,X1grid = np.meshgrid(np.linspace(X[:,0].cpu().min(),X[:,0].cpu().max(),101),np.linspace(X[:,1].cpu().min(),X[:,1].cpu().max(),101))\n",
    "\n",
    "X_grid = np.vstack((X0grid.ravel(),X1grid.ravel())).T\n",
    "X_grid = torch.from_numpy(X_grid)\n",
    "X_grid = X_grid.to(torch.float32)\n",
    "X_grid = X_grid.cuda()\n",
    "\n",
    "t = model(X_grid)\n",
    "out = F.softmax(t,dim=1)\n",
    "plt.contourf(X0grid,X1grid,out.cpu().detach().numpy()[:,1].reshape((101,101)),alpha=0.5)\n",
    "plt.scatter(X0[:,0],X0[:,1],c=y0)\n",
    "plt.scatter(X0_test[:,0],X0_test[:,1],c=y0_test,marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c $L_2$ Regularization for LFW\n",
    "As we've seen before, image data gives us an opportunity to look at the weights.  Because $L_2$ regularization is manipulating weights directly, this gives us an opportunity to get a sense of *what qualitative effect the regularization is having on the weights*.  Let's apply this to Labeled Faces in the Wild, which we're now used to seeing. \n",
    "\n",
    "As usual, the data manipulation ritual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fetch LFW dataset, ensuring that we have at least 50 images per class (i.e. per person)\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=25, resize=0.4)\n",
    "\n",
    "# Extract number of data points, and the height and width of the images for later reshaping\n",
    "m, h, w = lfw_people.images.shape\n",
    "n = h*w\n",
    "\n",
    "# Extract number of classes\n",
    "N = len(lfw_people.target_names)\n",
    "\n",
    "# Split the training and test set\n",
    "X,X_test,y,y_test = train_test_split(lfw_people.data,lfw_people.target)\n",
    "X/=255.\n",
    "X_test/=255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct a simple neural network, not dissimilar to the ones that you created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(n,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,N)\n",
    "        #self.l3 = nn.Linear(256,10)\n",
    "        \n",
    "        #self.dropout_1 = nn.Dropout(p=0.3)\n",
    "        #self.dropout_2 = nn.Dropout(p=0.3)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.sigmoid(a1)\n",
    "        #z1d = self.dropout_1(z1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "        #z2 = torch.relu(a2)\n",
    "        #z2d = self.dropout_2(z2)\n",
    "       # \n",
    "        #a3 = self.l3(z2d)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try training the model without L2 regularization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "gamma=0\n",
    "#gamma = 2e5\n",
    "\n",
    "epochs = 500\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        for p in model.parameters():\n",
    "            if p.dim()>1:\n",
    "                loss += gamma/(2*d.shape[0])*(p**2).mean()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)\n",
    "params = [p for p in model.l1.parameters()]\n",
    "W1_noreg = params[0].cpu().detach().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's try it with regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "gamma = 1e4\n",
    "\n",
    "epochs = 500\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        for p in model.parameters():\n",
    "            if p.dim()>1:\n",
    "                loss += gamma/(2*d.shape[0])*(torch.abs(p)).mean()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)\n",
    "params = [p for p in model.l1.parameters()]\n",
    "W1_L2 = params[0].cpu().detach().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the regularized model has both reduced training set accuracy *and* reduced test set accuracy.  This implies that our regularization is not very useful: that's because this dataset is, in fact, not very \"noisy\" (in the sense of multiple overlapping data points of different classes; image data is usually this way because it's so high dimensional).  However, if we plot some of the features being extracted in the form of extracted weight matrices, we can learn something about the effect of $L_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=2,ncols=6)\n",
    "fig.set_size_inches(10,4)\n",
    "for i in range(6):\n",
    "    axs[0,i].imshow(W1_noreg[:,np.random.randint(W1_noreg.shape[1])].reshape((h,w)))\n",
    "    axs[1,i].imshow(W1_L2[:,np.random.randint(W1_L2.shape[1])].reshape((h,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $L_2$ regularization penalizes large weights more than small ones, it has the tendency to ensure that all the weights are around the same size, which also means that it tends towards extracting larger, smoother features.\n",
    "\n",
    "## IC8B $L_1$ regularization\n",
    "We chose to regularize by penalizing the sum of squared weights (if you don't believe this, go back to the formula).  However, there are many other ways that we could exert pressure on the weights to behave one way or another.  One very interesting possibility is so-called $L_1$ regularization, which penalizes the $L_1$ norm of the weights.  What, you ask, is the $L_1$ norm?  It is the *sum of absolute values*:\n",
    "$$\n",
    "\\|W^{(l)}\\|_1 = \\sum_{i} \\sum_{j} |w_{ij}^{(l)}|.\n",
    "$$\n",
    "Superficially, it would seem that this would do the same thing as $L_2$ regularization (make the weights smaller), and it is true that it does have this property.  However, it has a very different effect on the *distribution* of weights.  **Implement $L_1$ regularization, using the code above as a starting point.  Discuss the qualitative effect that this form of regularization has on the resulting weight matrices.  HINT 1:(You'll want to reduce the value of $\\gamma$ to 10^4 for this).  Hint 2: (You might have to do a little bit of searching to find weight matrices that aren't all close to zero).  Hint 3: (Take the derivative of the $L_1$ norm.  At each iteration of gradient descent, how much are large weights reduced versus small weights?)\n",
    "\n",
    "## 3 More exotic regularizers\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.RandomRotation(30)#,transforms.RandomResizedCrop(224),transforms.RandomHorizontalFlip()]\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = x.reshape((28,28))\n",
    "            x = transforms.ToPILImage(x)\n",
    "            x = self.transform(x)\n",
    "            print(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "training_data = CustomTensorDataset([X,y],transform=transform)\n",
    "test_data = CustomTensorDataset([X_test,y_test])\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "batch_size_train = 256\n",
    "batch_size_test = 256\n",
    "\n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,)),\n",
    "                               lambda x: x.flatten()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('./data/', train=True, download=True,transform=train_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size_train,pin_memory=True,num_workers=0)\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,)),\n",
    "                               lambda x: x.flatten()])\n",
    "test_dataset = torchvision.datasets.MNIST('./data/', train=False, download=True,transform=test_transforms)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size_test,pin_memory=True,num_workers=0z)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n = 784\n",
    "N = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(n,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,N)\n",
    "        #self.l3 = nn.Linear(256,10)\n",
    "        \n",
    "        #self.dropout_1 = nn.Dropout(p=0.3)\n",
    "        #self.dropout_2 = nn.Dropout(p=0.3)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.sigmoid(a1)\n",
    "        #z1d = self.dropout_1(z1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "        #z2 = torch.relu(a2)\n",
    "        #z2d = self.dropout_2(z2)\n",
    "       # \n",
    "        #a3 = self.l3(z2d)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "        #print(d.shape)\n",
    "        d,t = d.cuda(),t.cuda()\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        d,t = d.cuda(),t.cuda()\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.utils.data.DataLoader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
